{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create and optimize a baseline Decision Tree model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## How to create and optimize a baseline Decision Tree model for Binary Classification\n",
      "[0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
      " 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0]\n",
      "Best Number Of Components: 5\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\n",
      "[0.61764706 0.64705882 0.65625   ]\n",
      "\n",
      "0.6403186274509803\n",
      "\n",
      "0.016464496130569113\n"
     ]
    }
   ],
   "source": [
    "## How to create and optimize a baseline Decision Tree model for Binary Classification\n",
    "def Snippet_152(): \n",
    "    print()\n",
    "    print(format('## How to create and optimize a baseline Decision Tree model for Binary Classification','*^82'))    \n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # load libraries\n",
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Load the iris flower data\n",
    "    dataset = datasets.make_classification(n_samples=100, n_features=20, n_informative=5, \n",
    "                n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, \n",
    "                weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, \n",
    "                scale=1.0, shuffle=True, random_state=None)\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    print(y)\n",
    "    \n",
    "    # Create an scaler object\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Create a pca object\n",
    "    pca = decomposition.PCA()\n",
    "    \n",
    "    # Create a logistic regression object with an L2 penalty\n",
    "    dtreeClf = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    # Create a pipeline of three steps. First, standardize the data.\n",
    "    # Second, tranform the data with PCA.\n",
    "    # Third, train a Decision Tree Classifier on the data.\n",
    "    pipe = Pipeline(steps=[('sc', sc), \n",
    "                           ('pca', pca), \n",
    "                           ('dtreeClf', dtreeClf)])\n",
    "    \n",
    "    # Create Parameter Space\n",
    "    # Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1)\n",
    "    n_components = list(range(1,X.shape[1]+1,1))\n",
    "    \n",
    "    # Create lists of parameter for DecisionTreeRegressor\n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_depth = [4,6,8,10]\n",
    "    \n",
    "    # Create a dictionary of all the parameter options \n",
    "    # Note has you can access the parameters of steps of a pipeline by using '__â€™\n",
    "    parameters = dict(pca__n_components=n_components,\n",
    "                      dtreeClf__criterion=criterion,\n",
    "                      dtreeClf__max_depth=max_depth)\n",
    "    \n",
    "    # Conduct Parameter Optmization With Pipeline\n",
    "    # Create a grid search object\n",
    "    clf = GridSearchCV(pipe, parameters)\n",
    "    \n",
    "    # Fit the grid search\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    # View The Best Parameters\n",
    "    print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "    print(); print(clf.best_estimator_.get_params()['dtreeClf'])\n",
    "    \n",
    "    # Use Cross Validation To Evaluate Model\n",
    "    CV_Result = cross_val_score(clf, X, y, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "    print(); print(CV_Result)\n",
    "    print(); print(CV_Result.mean())\n",
    "    print(); print(CV_Result.std())    \n",
    "    \n",
    "Snippet_152()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
